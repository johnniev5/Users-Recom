{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778f8407",
   "metadata": {},
   "source": [
    "对于用户推荐业务逻辑上的一些思考:\n",
    "    \n",
    "1. 对于新用户的推荐, 也就是用户冷启动的问题, 可以以下方面思考:\n",
    "    * 需要及时推荐的 - 可以基于排行榜进行推荐, 如用户关注量, 热门帖子等；\n",
    "    * 不需要及时推荐的 - 可以在用户进行过相关的行为后, 收集模型需要的这些用户数据, 再根据模型给到推荐, 不限制于基于post和user的推荐；\n",
    "2. 对于已经有过用户行为的用户的推荐, 比如已经有了用户关注和被关注, 或已经有了浏览帖子, 喜欢, 转发, 评论过帖子的:\n",
    "    * 我们优先基于用户感兴趣的post帖子的推荐, 这也是最根本, 最有效的推荐:\n",
    "        * 不基于模型的推荐, 推荐的优先顺序有: 用户评论过, 转发过, 喜欢过, 点击过用户->用户点击过用户, 用户评论过, 用户转发过->用户点击过用户, 用户评论过, 喜欢过->用户点击过用户, 用户评论过->用户点击过用户, 转发过, 喜欢过->用户点击过用户, 转发过->用户点击过用户, 喜欢过->用户点击过用户->用户评论过->转发过->喜欢过->用户浏览过\n",
    "        * 基于模型的推荐, 用户在有了点击帖子和浏览的记录后, 就可以将这些数据送入模型, 来预测用户是否会对浏览内的帖子感兴趣, 按照模型给定的可能会点击的概率从高到底进行帖子的推荐, 而当推荐的帖子被用户点击, 喜欢, 转发, 评论, 甚至是点击用户后, 可以再按照上面的逻辑再推荐用户；\n",
    "    * 在进行帖子推荐后推荐列表中会有一些推荐用户, 而如果推荐的用户数没有占满这个推荐列表时, 我们可以再使用基于用户的推荐来补充:\n",
    "        * 用户有了帖子的浏览记录, 可以使用基于帖子的推荐, 而用户有了关注的用户, 就可以使用基于用户推荐的模型, 这也我这边在做的NuralCF模型；\n",
    "        * 具体逻辑和方法如下:\n",
    "            * 简单逻辑:\n",
    "                * 直接使用待推荐的这个用户没有关注的用户列表送入模型, 得到打分, 按照这个打分的高低顺序给到相应的推荐；\n",
    "            * 复杂逻辑:\n",
    "                * 我们根据用户关注的用户列表, 使用模型得到这些关注关系的向量, 得到这些向量后, 再去库里找到与这些向量相似度接近的用户关系, 至于需要找多少个相近关系的, 参数可自己定义, 找到这部分用户关系后的逻辑为, 这些关系中被关注的用户, 是我这个要被推荐用户也关注过的, 且这些关系中的关注的这个用户的被关注用户是我没有关注的, 我们可以找到我与这些未被我关注的用户, 再送入模型, 得到我们之间关注的可能性, 按照模型打分的高低顺序给到推荐；\n",
    "                * 如果被推荐的用户列表未占满, 我们可以采用如下策略:\n",
    "                    * 对于这个我也关注的用户, 去找到他关注的列表, 送入模型, 得到对应的关系向量, 可以按照这部分我也关注的用户, 他们关注的我没有关注(排除上一步中的用户)的用户的关注可能的概率再排序给到推荐；\n",
    "                    * 还可以找我也关注的这个用户, 关注他们的用户, 他们关注的用户列表, 给到推荐, 逻辑与上相同；\n",
    "                * 如果基于第一步中我们没有找到我也关注的用户, 我们可以直接按照这个打分的高低, 将这些用户进行推荐, 如未占满推荐列表, 也可以按照上面的逻辑, 找被关注者关注的用户, 和关注被关注者关注的用户再进行相应的推荐\n",
    "            \n",
    "以上, 具体业务逻辑, 可以自行设计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524430d",
   "metadata": {},
   "source": [
    "以下参考相关文献和代码:\n",
    "\n",
    "1. 论文地址:\n",
    "    * https://arxiv.org/pdf/1708.05031.pdf\n",
    "2. 资料和代码:\n",
    "    * https://blog.csdn.net/wuzhongqiang/article/details/108985457\n",
    "    * https://www.cnblogs.com/sxzhou/p/14585324.html\n",
    "    * https://github.com/supkoon/neuralCF_tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1e4acb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:28:52.777357Z",
     "start_time": "2021-11-25T12:28:51.520881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 20:28:51.814038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ndcg_score, precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c442da49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:28:52.780425Z",
     "start_time": "2021-11-25T12:28:52.778616Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e428b3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:28:52.832869Z",
     "start_time": "2021-11-25T12:28:52.781374Z"
    }
   },
   "outputs": [],
   "source": [
    "class NeuralMF:\n",
    "    def __init__(self, num_users, num_items, latent_features=8, mlp_layers_units=[64,32,16,8]):\n",
    "        self.num_users = num_users\n",
    "        self.num_items = num_items\n",
    "        self.latent_features = latent_features\n",
    "        self.mlp_layers_units = mlp_layers_units\n",
    "\n",
    "        user_input = keras.layers.Input(shape=(1,), dtype='int32')\n",
    "        item_input = keras.layers.Input(shape=(1,), dtype='int32')\n",
    "\n",
    "        user_embedding_gmf = keras.layers.Embedding(self.num_users, self.latent_features,\n",
    "                                                    name = 'user_embedding_gmf')(user_input)\n",
    "        item_embedding_gmf = keras.layers.Embedding(self.num_items, self.latent_features,\n",
    "                                                    name = 'item_embedding_gmf')(item_input)\n",
    "        user_latent_gmf = keras.layers.Flatten()(user_embedding_gmf)\n",
    "        item_latent_gmf = keras.layers.Flatten()(item_embedding_gmf)\n",
    "\n",
    "        result_gmf = keras.layers.Multiply()([user_latent_gmf, item_latent_gmf])\n",
    "\n",
    "        user_embedding_mlp = keras.layers.Embedding(self.num_users, self.latent_features,\n",
    "                                                    name='user_embedding_mlp')(user_input)\n",
    "        item_embedding_mlp = keras.layers.Embedding(self.num_items, self.latent_features, \n",
    "                                                    name='item_embedding_mlp')(item_input)\n",
    "\n",
    "        user_latent_mlp = keras.layers.Flatten()(user_embedding_mlp)\n",
    "        item_latent_mlp = keras.layers.Flatten()(item_embedding_mlp)\n",
    "\n",
    "        result_mlp = keras.layers.concatenate([user_latent_mlp, item_latent_mlp])\n",
    "\n",
    "        for unit in self.mlp_layers_units:\n",
    "            layer = keras.layers.Dense(unit, activation='relu')\n",
    "            result_mlp =layer(result_mlp)\n",
    "\n",
    "        concat = keras.layers.concatenate([result_gmf, result_mlp])\n",
    "\n",
    "        output = keras.layers.Dense(1, name='output', activation='sigmoid')(concat)\n",
    "\n",
    "        self.model = keras.Model(inputs=[user_input, item_input], outputs=[output])\n",
    "\n",
    "    def get_model(self):\n",
    "        model = self.model\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dcec42a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:28:52.947826Z",
     "start_time": "2021-11-25T12:28:52.834210Z"
    }
   },
   "outputs": [],
   "source": [
    "class Metrics(keras.callbacks.Callback):\n",
    "    def __init__(self, valid_data):\n",
    "        super().__init__()\n",
    "        self.validation_data = valid_data\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_predict = (np.asarray(self.model.predict([\n",
    "            self.validation_data[0], self.validation_data[1]]))).round()\n",
    "        val_targ = self.validation_data[2]\n",
    "        _val_f1 = f1_score(val_targ, val_predict)\n",
    "        _val_recall = recall_score(val_targ, val_predict)\n",
    "        _val_precision = precision_score(val_targ, val_predict)\n",
    "        self.val_f1s.append(_val_f1)\n",
    "        self.val_recalls.append(_val_recall)\n",
    "        self.val_precisions.append(_val_precision)\n",
    "        print('- val_precision: %.4f - val_recall %.4f - val_f1: %.4f' %\n",
    "              (_val_precision, _val_recall, _val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dc20d31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:28:53.009676Z",
     "start_time": "2021-11-25T12:28:52.948799Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_dict(data, lst=['u_index_value', 'f_index_value']):\n",
    "    d = dict()\n",
    "    for idx, row in data[lst].iterrows():\n",
    "        d[(row[0], row[1])] = 1\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dadaec3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:28:53.046898Z",
     "start_time": "2021-11-25T12:28:53.010504Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data_instances(train, num_negatives, num_items):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    for (u, i) in train.keys():\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while train.__contains__((u, j)):\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3446052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:29:02.023517Z",
     "start_time": "2021-11-25T12:28:53.047774Z"
    }
   },
   "outputs": [],
   "source": [
    "follows = pd.read_csv('follows.csv')\n",
    "follows.drop('rank', axis=1, inplace=True)\n",
    "follows.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309722ff",
   "metadata": {},
   "source": [
    "用户uid和fid进行index编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b92f33ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:29:10.920694Z",
     "start_time": "2021-11-25T12:29:02.024460Z"
    }
   },
   "outputs": [],
   "source": [
    "uid_codes = follows.uid.drop_duplicates().reset_index()\n",
    "fid_codes = follows.fid.drop_duplicates().reset_index()\n",
    "uid_codes.rename(columns={'index':'uid_index'}, inplace=True)\n",
    "fid_codes.rename(columns={'index':'fid_index'}, inplace=True)\n",
    "uid_codes['u_index_value'] = list(uid_codes.index)\n",
    "fid_codes['f_index_value'] = list(fid_codes.index)\n",
    "follows = pd.merge(follows, uid_codes, how='left')\n",
    "follows = pd.merge(follows, fid_codes, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee59bb81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:35:24.752678Z",
     "start_time": "2021-11-25T12:29:10.921554Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(follows, test_size=0.1)\n",
    "train_dict, test_dict = get_data_dict(train), get_data_dict(test)\n",
    "train_set, test_set = get_data_instances(train_dict, 4, train['f_index_value'].max()), get_data_instances(test_dict, 4, test['f_index_value'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c1ada88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:35:33.578922Z",
     "start_time": "2021-11-25T12:35:24.754291Z"
    }
   },
   "outputs": [],
   "source": [
    "X1, X2 = np.array(train_set[0]), np.array(train_set[1])\n",
    "\n",
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = train_test_split(X1, X2, np.array(train_set[2]), test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a879376a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:35:34.528152Z",
     "start_time": "2021-11-25T12:35:33.579763Z"
    }
   },
   "outputs": [],
   "source": [
    "num_users = follows.u_index_value.nunique()\n",
    "num_items = follows.f_index_value.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "147638e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T12:35:34.585516Z",
     "start_time": "2021-11-25T12:35:34.529086Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 20:35:34.530294: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2021-11-25 20:35:34.530321: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2021-11-25 20:35:34.531021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2021-11-25 20:35:34.582528: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 2 GPUs\n",
      "2021-11-25 20:35:34.583074: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so.10.1\n",
      "2021-11-25 20:35:34.583897: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2021-11-25 20:35:34.583978: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "callbacks_list = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_acc',\n",
    "        patience=5),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True),\n",
    "    keras.callbacks.TensorBoard(\n",
    "        log_dir='logs/',\n",
    "        histogram_freq=0,\n",
    "        write_graph=True,\n",
    "        write_images=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67040d89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:18:24.858969Z",
     "start_time": "2021-11-25T12:35:34.586674Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 20:35:34.608589: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 20:35:34.608849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.609303: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.8GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2021-11-25 20:35:34.609445: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.609860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.8GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2021-11-25 20:35:34.609900: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-25 20:35:34.611268: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-25 20:35:34.611355: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-11-25 20:35:34.612553: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-25 20:35:34.612831: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-25 20:35:34.613935: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-25 20:35:34.614433: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-25 20:35:34.616671: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-25 20:35:34.616851: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.617602: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.618254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.618811: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.619300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2021-11-25 20:35:34.619737: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-25 20:35:34.759797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.760189: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.8GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2021-11-25 20:35:34.760292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.760659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA GeForce RTX 2080 computeCapability: 7.5\n",
      "coreClock: 1.8GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 417.23GiB/s\n",
      "2021-11-25 20:35:34.760689: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-25 20:35:34.760714: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
      "2021-11-25 20:35:34.760730: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
      "2021-11-25 20:35:34.760744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2021-11-25 20:35:34.760758: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2021-11-25 20:35:34.760772: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-11-25 20:35:34.760786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-11-25 20:35:34.760801: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-11-25 20:35:34.760849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.761227: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.761624: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.761998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:34.762368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1\n",
      "2021-11-25 20:35:34.762404: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-11-25 20:35:35.369532: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2021-11-25 20:35:35.369565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 1 \n",
      "2021-11-25 20:35:35.369572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N N \n",
      "2021-11-25 20:35:35.369576: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 1:   N N \n",
      "2021-11-25 20:35:35.369796: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:35.370268: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:35.370662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:35.371029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:35.371371: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 6417 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:01:00.0, compute capability: 7.5)\n",
      "2021-11-25 20:35:35.371638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:35.372032: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-25 20:35:35.372387: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 7257 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 2080, pci bus id: 0000:03:00.0, compute capability: 7.5)\n",
      "2021-11-25 20:35:35.372640: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 20:35:38.116950: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_437\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n",
      "2021-11-25 20:35:38.149304: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2021-11-25 20:35:38.149611: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3600000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 4 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 10 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 4 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 20:35:40.761463: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11/88670 [..............................] - ETA: 30:49 - loss: 0.6920 - acc: 0.5518"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 20:35:42.986590: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2021-11-25 20:35:42.986624: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2021-11-25 20:35:42.986697: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1415] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI_ERROR_INSUFFICIENT_PRIVILEGES\n",
      "2021-11-25 20:35:43.002932: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2021-11-25 20:35:43.005224: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2021-11-25 20:35:43.006655: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2021-11-25 20:35:43.008014: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/train/plugins/profile/2021_11_25_20_35_43\n",
      "2021-11-25 20:35:43.008785: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to logs/train/plugins/profile/2021_11_25_20_35_43/johnnie.trace.json.gz\n",
      "2021-11-25 20:35:43.021491: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: logs/train/plugins/profile/2021_11_25_20_35_43\n",
      "2021-11-25 20:35:43.024000: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to logs/train/plugins/profile/2021_11_25_20_35_43/johnnie.memory_profile.json.gz\n",
      "2021-11-25 20:35:43.024269: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/train/plugins/profile/2021_11_25_20_35_43Dumped tool data for xplane.pb to logs/train/plugins/profile/2021_11_25_20_35_43/johnnie.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/train/plugins/profile/2021_11_25_20_35_43/johnnie.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/train/plugins/profile/2021_11_25_20_35_43/johnnie.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/train/plugins/profile/2021_11_25_20_35_43/johnnie.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/train/plugins/profile/2021_11_25_20_35_43/johnnie.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88668/88670 [============================>.] - ETA: 0s - loss: 0.2927 - acc: 0.8973"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 20:58:11.812212: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_180958\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "88670/88670 [==============================] - 1367s 15ms/step - loss: 0.2927 - acc: 0.8973 - val_loss: 0.2430 - val_acc: 0.9136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 20:58:25.048838: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_201326\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- val_precision: 0.8698 - val_recall 0.6678 - val_f1: 0.7556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 21:00:44.277480: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/assets\n",
      "Epoch 2/100\n",
      "88670/88670 [==============================] - 1349s 15ms/step - loss: 0.2255 - acc: 0.9185 - val_loss: 0.2381 - val_acc: 0.9163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 21:23:16.452313: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_558295\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- val_precision: 0.8831 - val_recall 0.6700 - val_f1: 0.7620\n",
      "INFO:tensorflow:Assets written to: models/assets\n",
      "Epoch 3/100\n",
      "88670/88670 [==============================] - 1363s 15ms/step - loss: 0.2096 - acc: 0.9234 - val_loss: 0.2400 - val_acc: 0.9166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 21:48:24.266666: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_915004\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- val_precision: 0.8766 - val_recall 0.6783 - val_f1: 0.7648\n",
      "Epoch 4/100\n",
      "88670/88670 [==============================] - 1330s 15ms/step - loss: 0.2015 - acc: 0.9260 - val_loss: 0.2422 - val_acc: 0.9168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 22:12:58.561557: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1270071\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- val_precision: 0.8685 - val_recall 0.6876 - val_f1: 0.7676\n",
      "Epoch 5/100\n",
      "88670/88670 [==============================] - 1318s 15ms/step - loss: 0.1954 - acc: 0.9282 - val_loss: 0.2461 - val_acc: 0.9155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 22:37:20.447734: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1625138\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- val_precision: 0.8610 - val_recall 0.6885 - val_f1: 0.7651\n",
      "Epoch 6/100\n",
      "88670/88670 [==============================] - 1333s 15ms/step - loss: 0.1897 - acc: 0.9302 - val_loss: 0.2457 - val_acc: 0.9155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 23:01:55.922316: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_1980205\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- val_precision: 0.8651 - val_recall 0.6838 - val_f1: 0.7638\n",
      "Epoch 7/100\n",
      "88670/88670 [==============================] - 1320s 15ms/step - loss: 0.1854 - acc: 0.9317 - val_loss: 0.2493 - val_acc: 0.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 23:26:18.979046: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_2335272\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- val_precision: 0.8404 - val_recall 0.6962 - val_f1: 0.7615\n",
      "Epoch 8/100\n",
      "88670/88670 [==============================] - 1325s 15ms/step - loss: 0.1814 - acc: 0.9332 - val_loss: 0.2508 - val_acc: 0.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-25 23:50:42.595540: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_2690339\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- val_precision: 0.8396 - val_recall 0.6969 - val_f1: 0.7617\n",
      "Epoch 9/100\n",
      "88670/88670 [==============================] - 1347s 15ms/step - loss: 0.1782 - acc: 0.9344 - val_loss: 0.2542 - val_acc: 0.9118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 00:15:45.230685: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_3045406\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- val_precision: 0.8340 - val_recall 0.6975 - val_f1: 0.7596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fab0150fb80>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH = 512\n",
    "\n",
    "metrics = Metrics((X1_val, X2_val, y_val))\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = NeuralMF(num_users, num_items, 8, [64,32,16,8]).get_model()\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['acc'])\n",
    "\n",
    "model.fit([X1_train, X2_train],\n",
    "          y_train,\n",
    "          batch_size=BATCH,\n",
    "          epochs=100, verbose=1, validation_data=([X1_val, X2_val], y_val),\n",
    "          callbacks=[metrics] + callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81a04f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:18:24.865801Z",
     "start_time": "2021-11-25T16:18:24.859859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding_mlp (Embedding)  (None, 1, 8)         8185288     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding_mlp (Embedding)  (None, 1, 8)         10979616    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 8)            0           user_embedding_mlp[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 8)            0           item_embedding_mlp[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16)           0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           1088        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "user_embedding_gmf (Embedding)  (None, 1, 8)         8185288     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "item_embedding_gmf (Embedding)  (None, 1, 8)         10979616    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           2080        dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8)            0           user_embedding_gmf[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 8)            0           item_embedding_gmf[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 16)           528         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 8)            0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 8)            136         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16)           0           multiply[0][0]                   \n",
      "                                                                 dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "output (Dense)                  (None, 1)            17          concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 38,333,657\n",
      "Trainable params: 38,333,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7155f90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:18:38.841295Z",
     "start_time": "2021-11-25T16:18:24.866809Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 00:18:25.365825: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_3203255\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10947/10947 [==============================] - 13s 1ms/step - loss: 0.0000e+00 - acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate([np.array(test_set[0]), np.array(test_set[1])], batch_size=BATCH, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4164d385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:18:38.845103Z",
     "start_time": "2021-11-25T16:18:38.842558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcadf5b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:22.841450Z",
     "start_time": "2021-11-25T16:18:38.845914Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 00:18:39.323974: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_3225643\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict([np.array(test_set[0]), np.array(test_set[1])]).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6c6acbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:23.523537Z",
     "start_time": "2021-11-25T16:21:22.846094Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9068501753847581"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(np.array(test_set[2]), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85117ffb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:25.665129Z",
     "start_time": "2021-11-25T16:21:23.524410Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8149611765473107"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(np.array(test_set[2]), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c10c30f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:27.819452Z",
     "start_time": "2021-11-25T16:21:25.666101Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6911863360464743"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(np.array(test_set[2]), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14466873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:29.988806Z",
     "start_time": "2021-11-25T16:21:27.820590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7479878629787061"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(np.array(test_set[2]), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85eeac15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:29.991437Z",
     "start_time": "2021-11-25T16:21:29.989645Z"
    }
   },
   "outputs": [],
   "source": [
    "# ndcg_score([test_set[2]], [[int(v[0]) for v in y_pred]], k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f23104",
   "metadata": {},
   "source": [
    "简单逻辑实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "238ecc41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:29.998377Z",
     "start_time": "2021-11-25T16:21:29.992255Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40dfb574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:30.074846Z",
     "start_time": "2021-11-25T16:21:29.999251Z"
    }
   },
   "outputs": [],
   "source": [
    "random_test_id = random.choice(follows['u_index_value'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff5876be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:30.088308Z",
     "start_time": "2021-11-25T16:21:30.075994Z"
    }
   },
   "outputs": [],
   "source": [
    "random_test_df = follows[follows['u_index_value'] == random_test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1bec2fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:21:30.099699Z",
     "start_time": "2021-11-25T16:21:30.089284Z"
    }
   },
   "outputs": [],
   "source": [
    "test_fids = set(follows[follows['u_index_value'] == random_test_id]['f_index_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79f8a0cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:43:18.050536Z",
     "start_time": "2021-11-26T01:43:18.048260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1372450"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(recom_fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19ec6f58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:22:12.680440Z",
     "start_time": "2021-11-25T16:22:12.673435Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>fid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>884310</td>\n",
       "      <td>983</td>\n",
       "      <td>0.999738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>874</th>\n",
       "      <td>884310</td>\n",
       "      <td>874</td>\n",
       "      <td>0.999605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>902</th>\n",
       "      <td>884310</td>\n",
       "      <td>902</td>\n",
       "      <td>0.999510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338</th>\n",
       "      <td>884310</td>\n",
       "      <td>1338</td>\n",
       "      <td>0.999485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1348</th>\n",
       "      <td>884310</td>\n",
       "      <td>1348</td>\n",
       "      <td>0.999455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>884310</td>\n",
       "      <td>939</td>\n",
       "      <td>0.999443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>884310</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.999436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1849</th>\n",
       "      <td>884310</td>\n",
       "      <td>1850</td>\n",
       "      <td>0.999432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1746</th>\n",
       "      <td>884310</td>\n",
       "      <td>1747</td>\n",
       "      <td>0.999319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>958</th>\n",
       "      <td>884310</td>\n",
       "      <td>958</td>\n",
       "      <td>0.999313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>884310</td>\n",
       "      <td>774</td>\n",
       "      <td>0.999313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>884310</td>\n",
       "      <td>773</td>\n",
       "      <td>0.999242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>884310</td>\n",
       "      <td>336</td>\n",
       "      <td>0.999236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>884310</td>\n",
       "      <td>1019</td>\n",
       "      <td>0.999182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>884310</td>\n",
       "      <td>1409</td>\n",
       "      <td>0.999161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>884310</td>\n",
       "      <td>1643</td>\n",
       "      <td>0.999161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>884310</td>\n",
       "      <td>1417</td>\n",
       "      <td>0.999148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>884310</td>\n",
       "      <td>778</td>\n",
       "      <td>0.999098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1702</th>\n",
       "      <td>884310</td>\n",
       "      <td>1703</td>\n",
       "      <td>0.999061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>884310</td>\n",
       "      <td>1616</td>\n",
       "      <td>0.999054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         uid   fid     score\n",
       "983   884310   983  0.999738\n",
       "874   884310   874  0.999605\n",
       "902   884310   902  0.999510\n",
       "1338  884310  1338  0.999485\n",
       "1348  884310  1348  0.999455\n",
       "939   884310   939  0.999443\n",
       "1055  884310  1055  0.999436\n",
       "1849  884310  1850  0.999432\n",
       "1746  884310  1747  0.999319\n",
       "958   884310   958  0.999313\n",
       "774   884310   774  0.999313\n",
       "773   884310   773  0.999242\n",
       "336   884310   336  0.999236\n",
       "1019  884310  1019  0.999182\n",
       "1409  884310  1409  0.999161\n",
       "1643  884310  1643  0.999161\n",
       "1417  884310  1417  0.999148\n",
       "778   884310   778  0.999098\n",
       "1702  884310  1703  0.999061\n",
       "1616  884310  1616  0.999054"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recom_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3958f2",
   "metadata": {},
   "source": [
    "复杂逻辑实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73f5eac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:22:12.701123Z",
     "start_time": "2021-11-25T16:22:12.681242Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "intermediate_layer_model = Model(inputs=model.input, outputs=model.get_layer('concatenate_1').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02b2cf00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:24:50.833077Z",
     "start_time": "2021-11-25T16:22:12.702131Z"
    }
   },
   "outputs": [],
   "source": [
    "intermediate_output = intermediate_layer_model.predict([np.array(follows['u_index_value']), np.array(follows['f_index_value'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df8058de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:24:52.831574Z",
     "start_time": "2021-11-25T16:24:50.841664Z"
    }
   },
   "outputs": [],
   "source": [
    "data = {**train_dict, **test_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "181eec22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:24:59.795660Z",
     "start_time": "2021-11-25T16:24:52.832464Z"
    }
   },
   "outputs": [],
   "source": [
    "vec_dict = dict()\n",
    "for idx, vec in zip(data.keys(), intermediate_output): \n",
    "    vec_dict[idx] = vec\n",
    "    \n",
    "index_dict = dict()\n",
    "for n, idx in enumerate(data.keys()): \n",
    "    index_dict[n] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bc1d8dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:24:59.826257Z",
     "start_time": "2021-11-25T16:24:59.796458Z"
    }
   },
   "outputs": [],
   "source": [
    "test_intermediate_output = intermediate_layer_model.predict([np.array(random_test_df['u_index_value']), np.array(random_test_df['f_index_value'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4ae208d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:25:32.437020Z",
     "start_time": "2021-11-25T16:24:59.827175Z"
    }
   },
   "outputs": [],
   "source": [
    "vecs = []\n",
    "for i in vec_dict.values():\n",
    "    vecs.append(list(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39edb2c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:25:32.440091Z",
     "start_time": "2021-11-25T16:25:32.437920Z"
    }
   },
   "outputs": [],
   "source": [
    "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1, reduction=tf.keras.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9510b50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:25:32.461121Z",
     "start_time": "2021-11-25T16:25:32.441072Z"
    }
   },
   "outputs": [],
   "source": [
    "test_vec = [list(v) for v in test_intermediate_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e865b61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:25:45.843494Z",
     "start_time": "2021-11-25T16:25:32.461986Z"
    }
   },
   "outputs": [],
   "source": [
    "data_vecs = [list(v) for v in vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e37dcf53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:25:45.849111Z",
     "start_time": "2021-11-25T16:25:45.844447Z"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6bd50be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:27:31.145401Z",
     "start_time": "2021-11-25T16:25:45.849979Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 200\n",
    "\n",
    "max_indexes = []\n",
    "for vec in test_vec:\n",
    "    m = list(cosine_loss([vec], data_vecs).numpy())\n",
    "    max_number = heapq.nlargest(n, m) \n",
    "    max_index = []\n",
    "    for t in max_number:\n",
    "        index = m.index(t)\n",
    "        max_index.append(index)\n",
    "        m[index] = 0\n",
    "    max_indexes.append(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "086f38f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:27:31.149083Z",
     "start_time": "2021-11-25T16:27:31.146215Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_dup_indexes = []\n",
    "for idx in max_indexes:\n",
    "    if not idx in drop_dup_indexes:\n",
    "        drop_dup_indexes.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8845c7ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-25T16:27:31.217280Z",
     "start_time": "2021-11-25T16:27:31.149998Z"
    }
   },
   "outputs": [],
   "source": [
    "recom_list = []\n",
    "for idxs in drop_dup_indexes:\n",
    "    for idx in idxs:\n",
    "        if (index_dict[idx][1] in test_fids) and (index_dict[idx][0] not in test_fids) and (index_dict[idx][0] != random_test_id):\n",
    "            recom_list.append(index_dict[idx][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e7bccb7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:38:17.519463Z",
     "start_time": "2021-11-26T01:38:17.517433Z"
    }
   },
   "outputs": [],
   "source": [
    "temp_list = copy.deepcopy(recom_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f910d44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:38:18.495826Z",
     "start_time": "2021-11-26T01:38:18.493228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1021762]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recom_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2268b48a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:39:14.822999Z",
     "start_time": "2021-11-26T01:38:19.993783Z"
    }
   },
   "outputs": [],
   "source": [
    "K = 20\n",
    "iter_count = 0\n",
    "\n",
    "while len(recom_list) < K and len(recom_list) > 0:\n",
    "    if iter_count < 10:\n",
    "        t_list = []\n",
    "        if len(temp_list) != 0:\n",
    "            for fid in temp_list:\n",
    "                temp = follows[follows['u_index_value'] == fid]\n",
    "                temp_intermediate_output = intermediate_layer_model.predict([np.array(temp['u_index_value']), np.array(temp['f_index_value'])])\n",
    "                temp_vec = [list(v) for v in temp_intermediate_output]\n",
    "                max_indexes = []\n",
    "                for vec in temp_vec:\n",
    "                    m = list(cosine_loss([vec], data_vecs).numpy())\n",
    "                    max_number = heapq.nlargest(n, m) \n",
    "                    max_index = []\n",
    "                    for t in max_number:\n",
    "                        index = m.index(t)\n",
    "                        max_index.append(index)\n",
    "                        m[index] = 0\n",
    "                    max_indexes.append(max_index)\n",
    "                drop_dup_indexes = []\n",
    "                for idx in max_indexes:\n",
    "                    if not idx in drop_dup_indexes:\n",
    "                        drop_dup_indexes.append(idx)\n",
    "                temp_fids = set(follows[follows['u_index_value'] == fid]['f_index_value'])\n",
    "                for idxs in drop_dup_indexes:\n",
    "                    for idx in idxs:\n",
    "                        if (index_dict[idx][1] in temp_fids) and (index_dict[idx][0] not in temp_fids) and (index_dict[idx][0] != fid) and (index_dict[idx][0] != random_test_id):\n",
    "                            recom_list.append(index_dict[idx][0])\n",
    "                            t_list.append(index_dict[idx][0])\n",
    "                temp_list = copy.deepcopy(t_list)\n",
    "    else:\n",
    "        break\n",
    "    iter_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25f1cc43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:39:15.040047Z",
     "start_time": "2021-11-26T01:39:14.824023Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 09:39:14.942416: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_3794842\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    }
   ],
   "source": [
    "if len(recom_list) != 0:\n",
    "    pred = model.predict([np.array([random_test_id] * len(recom_list)), np.array(recom_list)])\n",
    "    recom_df = pd.DataFrame({'uid': [random_test_id] * len(recom_list), 'fid': (recom_list), 'score': [p[0] for p in pred]})\n",
    "    recom_df.sort_values('score', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f89cef7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:39:15.048043Z",
     "start_time": "2021-11-26T01:39:15.041604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>fid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>884310</td>\n",
       "      <td>1021762</td>\n",
       "      <td>0.205373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      uid      fid     score\n",
       "0  884310  1021762  0.205373"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recom_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8a5d6f39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:39:15.063172Z",
     "start_time": "2021-11-26T01:39:15.049069Z"
    }
   },
   "outputs": [],
   "source": [
    "recom_list = []\n",
    "if len(recom_list) <= K:\n",
    "    for idxs in drop_dup_indexes:\n",
    "        for idx in idxs:\n",
    "            if index_dict[idx][1] not in test_fids:\n",
    "                recom_list.append([random_test_id, index_dict[idx][1]])\n",
    "            if index_dict[idx][0] not in test_fids:\n",
    "                recom_list.append([random_test_id, index_dict[idx][0]])\n",
    "                \n",
    "drop_dup_recom = []\n",
    "for r in recom_list:\n",
    "    if not r in drop_dup_recom:\n",
    "        drop_dup_recom.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d2b11fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:39:15.211790Z",
     "start_time": "2021-11-26T01:39:15.064087Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-26 09:39:15.103215: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:656] In AUTO-mode, and switching to DATA-based sharding, instead of FILE-based sharding as we cannot find appropriate reader dataset op(s) to shard. Error: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_3795026\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([np.array(drop_dup_recom)[:,0], np.array(drop_dup_recom)[:,1]])\n",
    "\n",
    "recom_df = pd.DataFrame({'uid': list(np.array(drop_dup_recom)[:,0]), 'fid': list(np.array(drop_dup_recom)[:,1]), 'score': [p[0] for p in pred]})\n",
    "\n",
    "recom_df.sort_values('score', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f29becf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-26T01:39:15.217750Z",
     "start_time": "2021-11-26T01:39:15.212566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>fid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>884310</td>\n",
       "      <td>874</td>\n",
       "      <td>0.999605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>884310</td>\n",
       "      <td>939</td>\n",
       "      <td>0.999443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>884310</td>\n",
       "      <td>1055</td>\n",
       "      <td>0.999436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>884310</td>\n",
       "      <td>1850</td>\n",
       "      <td>0.999432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>884310</td>\n",
       "      <td>1019</td>\n",
       "      <td>0.999182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>884310</td>\n",
       "      <td>1417</td>\n",
       "      <td>0.999148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>884310</td>\n",
       "      <td>778</td>\n",
       "      <td>0.999098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>884310</td>\n",
       "      <td>1703</td>\n",
       "      <td>0.999061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>884310</td>\n",
       "      <td>312</td>\n",
       "      <td>0.999017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>884310</td>\n",
       "      <td>745</td>\n",
       "      <td>0.998678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>884310</td>\n",
       "      <td>860</td>\n",
       "      <td>0.998644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>884310</td>\n",
       "      <td>3379</td>\n",
       "      <td>0.998460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>884310</td>\n",
       "      <td>323</td>\n",
       "      <td>0.998123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        uid   fid     score\n",
       "43   884310   874  0.999605\n",
       "287  884310   939  0.999443\n",
       "34   884310  1055  0.999436\n",
       "139  884310  1850  0.999432\n",
       "115  884310  1019  0.999182\n",
       "47   884310  1417  0.999148\n",
       "343  884310   778  0.999098\n",
       "105  884310  1703  0.999061\n",
       "321  884310   312  0.999017\n",
       "226  884310   745  0.998678\n",
       "259  884310   860  0.998644\n",
       "151  884310  3379  0.998460\n",
       "181  884310   323  0.998123"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recom_df.head(int(K-len(recom_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0463c1",
   "metadata": {},
   "source": [
    "以上测试结果:\n",
    "1. 在使用GPU加速的情况下, 几十, 几百万的用户推荐都很及时, 可以直接使用简单逻辑来推荐, 而对于复杂逻辑看来结果并不那么好；\n",
    "2. 对于没有GPU的情况下, 可以进一步针对待推荐列表进行过滤, 比如过滤掉非活跃用户, 过滤掉黑名单用户, 过滤掉被举报用户等, 来缩小待推荐列表以尽量实现快速推荐；"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
